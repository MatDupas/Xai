%
% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}

% For links
\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}
\usepackage{url}
% originally to compile w/ correct hyperlinks, needed: 
% latex -> bibtex -> latex -> latex -> dvi2ps -> ps2pdf
% now PDFLaTeX seems to work.
\usepackage{subcaption}

% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.

% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{\url{https://fatconference.org/2020/callfortutorials.html}}
\acmYear{}
\setcopyright{none}
\acmConference[ACM FAT* Call for Tutorials -- \textit{UNDER REVIEW}]{ACM FAT* Conference 2020}{Barcelona}{2020}
\acmBooktitle{ACM FAT* Conference, Jan. 27\textsuperscript{th} -- 30\textsuperscript{th}, 2020, Barcelona, Spain}
\acmPrice{}
\acmDOI{}
\acmISBN{}

% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}


% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}


% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}

%two part definition
\newcommand{\twopartdef}[4]
{
	\left\{
		\begin{array}{ll}
			#1 & \mbox{if } #2 \\
			#3 & \mbox{if } #4
		\end{array}
	\right.
}

% end of the preamble, start of the body of the document source.
\begin{document}

% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title[Responsible XAI]{\huge{Hands-On Tutorial: Guidelines for Responsible Use of\\ Explainable Machine Learning}}


% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Patrick Hall}
\email{patrick.hall@h2o.ai}
\affiliation{\institution{H2O.ai}}
\affiliation{\institution{George Washington University}}

\author{Navdeep Gill}
\email{navdeep.gill@h2o.ai}
\affiliation{\institution{H2O.ai}}

\author{Nicholas Schmidt }
\email{nschmidt@bldsllc.com}
\affiliation{\institution{BLDS, LLC}}

\renewcommand{\shortauthors}{Hall, Gill, \& Schmidt}

%
% The abstract is a short summary of the work to be presented in the article.
%\begin{abstract}
%\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%
%\begin{CCSXML}
%	<ccs2012>

%		<concept>
%			<concept_id>10003120.10003145.10003147.10010365</concept_id>
%			<concept_desc>Human-centered computing~Visual analytics</concept_desc>
%			<concept_significance>300</concept_significance>
%		</concept>		

%		<concept>
%			<concept_id>10010147.10010257.10010258.10010259.10010263</concept_id>
%			<concept_desc>Computing methodologies~Supervised learning by classification</concept_desc>
%			<concept_significance>300</concept_significance>
%		</concept>

%		<concept>
%			<concept_id>10010147.10010257.10010258.10010259.10010264</concept_id>
%			<concept_desc>Computing methodologies~Supervised learning by regression</concept_desc>
%			<concept_significance>300</concept_significance>
%		</concept>
		
%		<concept>
%			<concept_id>10010147.10010257.10010293.10010307</concept_id>
%			<concept_desc>Computing methodologies~Learning linear models</concept_desc>
%			<concept_significance>300</concept_significance>
%		</concept>
		
%		<concept>
%			<concept_id>10010147.10010257.10010293.10003660</concept_id>
%			<concept_desc>Computing methodologies~Classification and regression trees</concept_desc>
%			<concept_significance>300</concept_significance>
%		</concept>

%		<concept>
%			<concept_id>10010147.10010257.10010321.10010333</concept_id>
%			<concept_desc>Computing methodologies~Ensemble methods</concept_desc>
%			<concept_significance>300</concept_significance>
%		</concept>		
				
%	</ccs2012>
%\end{CCSXML}

%\ccsdesc[300]{Human-centered computing~Visual analytics}
%\ccsdesc[300]{Computing methodologies~Supervised learning by classification}
%\ccsdesc[300]{Computing methodologies~Supervised learning by regression}
%\ccsdesc[300]{Computing methodologies~Learning linear models}
%\ccsdesc[300]{Computing methodologies~Classification and regression trees}
%\ccsdesc[300]{Computing methodologies~Ensemble methods}

%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
%\keywords{Machine learning, interpretability, explanations, transparency,\\ FATML, XAI}


% This command processes the author and affiliation and title information and builds
% the first part of the formatted document.
\maketitle

%-------------------------------------------------------------------------------
\section*{Introduction \& Agenda}
%-------------------------------------------------------------------------------

Explainable machine learning (ML) enables human learning from ML, human appeal of automated model decisions, regulatory compliance, and white-hat hacking and security audits of ML models.\footnote{In the U.S., interpretable models, explainable ML, and model documentation they enable may be required under the Civil Rights Acts of 1964 and 1991, the Americans with Disabilities Act, the Genetic Information Nondiscrimination Act, the Health Insurance Portability and Accountability Act, the Equal Credit Opportunity Act (ECOA), the Fair Credit Reporting Act (FCRA), the Fair Housing Act, Federal Reserve SR 11-7, and the European Union (EU) Greater Data Privacy Regulation (GDPR) Article 22 \cite{ff_interpretability}. \label{fn:regs}} \textsuperscript{,}\footnote{For security applications, see for instance: \url{https://www.oreilly.com/ideas/proposals-for-model-vulnerability-and-security}.\label{fn:regs}} Explainable ML (i.e. e\textit{x}plainable \textit{a}rtificial \textit{i}ntelligence or XAI) has been implemented in numerous open source and commercial packages and explainable ML is also an important, mandatory, or embedded aspect of commercial predictive modeling in industries like financial services.\footnote{Like H2O-3, XGBoost, and various other Python and R packages. See: \url{https://github.com/jphall663/awesome-machine-learning-interpretability} for a longer, curated list of relevant open source software packages.}\textsuperscript{,}\footnote{For instance  Datarobot, H2O Driverless AI, SAS Visual Data Mining and Machine Learning, Zest AutoML, and likely several others.}\textsuperscript{,}\footnote{For instance, ``Deep Insights into Explainability and Interpretability of Machine Learning Algorithms and Applications to Risk Management,'' Jie Chen, Wells Fargo Corporate Model Risk, \url{https://ww2.amstat.org/meetings/jsm/2019/onlineprogram/AbstractDetails.cfm?abstractid=303053}. \label{fn:Chen}} However, like many technologies, explainable ML can be misused and abused, particularly as a faulty safeguard for harmful black-boxes, e.g. \textit{fairwashing}, and for other malevolent purposes like stealing models or sensitive training data \cite{fair_washing}, \cite{please_stop}, \cite{membership_inference}, \cite{model_stealing}. In an effort to begin establishing best practices for this already in-flight technology and also to promote nuanced discussion, this tutorial presents:\\
\begin{itemize}
\item Definitions and examples related to explainable ML. \textbf{(Section \ref{sec:intro}; 20 mins.)}
\item Responsible use guidelines and corollaries:
\begin{itemize}
	\item Use explanations to enable understanding directly (and trust as a side-effect). \textbf{(Section \ref{sec:trust}; 40 mins.)}
	\item Learn how explainable ML is used for nefarious purposes. \textbf{(Section \ref{sec:nefarious}; 30 mins.)}
	\item Augment surrogate models with direct explanations.\\\textbf{(Section \ref{sec:surrogate}; 30 mins.)}
	\item Use fully transparent ML mechanisms for high-stakes applications. \textbf{(Section \ref{sec:white_box}; 50 mins.)}
\end{itemize}
\item Conclusion: a holistic approach to ML \textbf{(Section \ref{sec:conclusion}; 10 mins.)}
\end{itemize}
\noindent Total time: 180 mins.

\section{Definitions \& Examples} \label{sec:intro}

While explainable ML practitioners have seemingly not yet adopted a clear taxonomy of concepts or a precise vocabulary, many authors have grappled with a variety of general concepts related to interpretability and explanations, e.g. \citet{guidotti2018survey}, \citet{lipton1}, \citet{molnar}, \citet{murdoch2019interpretable}), and \citet{weller2017challenges}. To avoid ambiguity, this section addresses the terms and phrases \textit{interpretable}, \textit{explanation}, \textit{explainable ML}, \textit{interpretable models}, \textit{model debugging techniques}, and \textit{fairness techniques}.

\subsection{Interpretable \& Explanation}

This text selects for an internal definition of \textit{interpretable}, ``the ability to explain or to present in understandable terms to a human,'' from \citet{been_kim1}. In the context of machine learning, Sameer Singh, co-inventor of the popular local interpretable model-agnostic explanations (LIME) technique, defines an \textit{explanation} as, ``a collection of visual and/or interactive artifacts that provide a user with sufficient description of the model behavior to accurately perform tasks like evaluation, trusting, predicting, or improving the model.''\footnote{``Proposed Guidelines for Responsible Use of Explainable Machine Learning,'' Patrick Hall, H2O.ai, \url{https://github.com/jphall663/kdd_2019}.} From \citet{gilpin2018explaining}, ``when you can no longer keep asking why,'' will serve as an internal working definition of a \textit{good explanation}. These three thoughtful characterizations link explanation to some ML process being interpretable and also provide an abstract objective for any given explanatory task. 

\subsection{Explainable ML \& Interpretable Models }

Herein \textit{explainable ML} means mostly post-hoc analysis and techniques used to understand trained model mechanisms or predictions. Examples of common explainable ML techniques include:

\begin{itemize}
\item Local and global feature importance methods, e.g. Shapley values and derivative-based feature attribution \cite{grad_attr} \cite{keinan2004fair}, \cite{shapley}, \cite{shapley1988shapley}, \cite{kononenko2010efficient}.
\item Local and global model-agnostic surrogate models, e.g. surrogate decision trees and LIME \cite{dt_surrogate2}, \cite{viper}, \cite{dt_surrogate1}, \cite{lime-sup}, \cite{lime}, \cite{wf_xnn}. 
\item Local and global visualizations of model predictions, e.g. accumulated local effect (ALE) plots, 1- and 2-dimensional partial dependence plots, and individual conditional expectation (ICE) plots \cite{ale_plot}, \cite{esl}, \cite{ice_plots}.
\end{itemize}  

Although difficult to quantify, credible research efforts into measures of interpretability are underway \cite{friedler2019assessing}, \cite{molnar2019quantifying}. The ability to measure degrees of interpretability implies it's not a binary, on-off quantity.\footnote{See: Figure 3 in ``Quantifying Interpretability of Arbitrary Machine Learning Models Through Functional Decomposition'' (\citet{molnar2019quantifying}).} Here unconstrained, traditional black-box ML models, such as multilayer perceptron (MLP) neural networks and gradient boosting machines (GBMs), are said to be directly uninterpretable, potentially unsafe for use on humans, but not necessarily completely unexplainable. In this tutorial \textit{interpretable models} or ``white-box'' models will include linear models, decision trees, rule-based models, constrained or Bayesian variants of traditional black-box ML models, or novel types of models designed to be directly interpretable. Examples of sophisticated and interpretable-by-design modeling techniques include explainable neural networks (XNNs), explainable boosting machines (EBMs, a.k.a. GA2M), monotonically constrained GBMs, scalable Bayesian rule lists, or super-sparse linear integer models (SLIMs), \cite{slim}, \cite{wf_xnn}, \cite{sbrl}.\footnote{As implemented in the interpret library: \url{https://github.com/microsoft/interpret}.}\textsuperscript{,}\footnote{As implemented in XGBoost (\url{https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html}) or H2O-3 (\url{https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/H2O_tutorial_gbm_monotonicity.ipynb}).}\textsuperscript{,}\footnote{And other methods, e.g. \url{https://users.cs.duke.edu/~cynthia/papers.html}.}

\subsection{Model Debugging Techniques}

Herein \textit{model debugging techniques} refer to methods for testing ML models that increase trust in model mechanisms and predictions. Examples of model debugging techniques include model assertions, ML security audits, variants of sensitivity (i.e. ``what-if?'') analysis, variants of residual analysis, and units test used to verify the accuracy or security of ML models \cite{modeltracker}, \cite{kangdebugging}.\footnote{And similar methods, e.g. \url{https://debug-ml-iclr2019.github.io/}.} Model debugging should also include remediating any discovered errors or vulnerabilities. 

\subsection{Fairness Techniques}

For this tutorial, \textit{fairness techniques} are used to diagnose and remediate unwanted sociological bias in ML models. Diagnosis approaches include disparate impact testing and other tests for unwanted sociological bias \cite{feldman2015certifying}. Remediation methods tend to involve model selection by minimization of bias, preprocessing training data, e.g. reweighing (\citet{kamiran2012data}), training unbiased models, e.g. adversarial de-biasing (\citet{zhang2018mitigating}), or post-processing model predictions, e.g. by equalizing odds (\citet{hardt2016equality}).\footnote{And similar methods, e.g. \url{http://www.fatml.org/resources/relevant-scholarship}.} 
i have like 300
\section{Guidelines}

Four guidelines are presented and discussed in Sections \ref{sec:trust} -- \ref{sec:white_box} to assist practitioners in avoiding any unintentional misuse or in identifying any intentional abuse of explainable ML techniques. Important corollaries to the guidelines are also highlighted and simple, reproducible software examples accompany the guidelines to avoid hypothetical reasoning whenever possible. 

\subsection{Guideline: Use Explanations to Enable Understanding Directly.} \label{sec:trust}

\begin{figure*}[htb!]
	\begin{subfigure}{.4\textwidth} \centering
  		\includegraphics[height=0.8\linewidth, width=0.75\linewidth]{img/global_shap.png}
  		\caption{Global Shapley feature importance for $g_{\text{GBM}}$.}
  		\label{fig:global_shap}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth} \centering
		\vspace{5pt}\includegraphics[width=0.85\linewidth]{img/resid.png}
  		\caption{$g_{\text{GBM}}$ deviance residuals and predictions by \texttt{PAY\_0}.}
  		\label{fig:resid}
	\end{subfigure}
	\vspace{-8pt}
	\caption{An unconstrained GBM probability of default model, $g_{\text{GBM}}$, generally over-emphasizes the importance of the input feature \texttt{PAY\_0}, a customer's most recent repayment status. $g_{\text{GBM}}$ produces large positive residuals when \texttt{PAY\_0} indicates on-time payments (\texttt{PAY\_0} $\leq$ 1) and large negative residuals when \texttt{PAY\_0} indicates late payments (\texttt{PAY\_0} $>$ 1). Combining explanatory and debugging techniques shows that $g_{\text{GBM}}$ is explainable, but probably not trustworthy. Code to replicate Figure \ref{fig:global_shap_resid} is available here: \url{https://github.com/h2oai/xai_guidelines}.} 
	\label{fig:global_shap_resid}
\end{figure*}

While they are likely necessary for trust in many cases, explanations are certainly not sufficient for trust in all cases. Explanation, as a general idea, is related more directly to understanding and transparency than to trust.\footnote{The Merriam-Webster definition of \textit{explain}, accessed May 8\textsuperscript{th} 2019, does not mention \textit{trust}: \url{https://www.merriam-webster.com/dictionary/explain}.} Explanations promote understanding directly, and only increase trust if the explanation-facilitated understanding of a model is acceptable to human consumers. Simply put, one can understand and explain a model without trusting it. One can also trust a model and not be able to understand or explain it. 

\begin{itemize}

\item \textbf{Explanation \& understanding without trust}: In Figure \ref{fig:global_shap_resid}, global Shapley explanations and residual analysis identify a pathology in an unconstrained GBM model, $g_{\text{GBM}}$, trained on the UCI credit card dataset \cite{uci}. $g_{\text{GBM}}$ over emphasizes the input feature \texttt{PAY\_0}, or a customer's most recent repayment status. Due to over-emphasis of \texttt{PAY\_0}, $g_{\text{GBM}}$ is often unable to predict on-time payment if recent payments are delayed (\texttt{PAY\_0} $>$ 1), causing large negative residuals. $g_{\text{GBM}}$ is also often unable to predict default if recent payments are made on-time (\texttt{PAY\_0} $\leq$ 1), causing large positive residuals. In this example scenario, $g_{\text{GBM}}$ is explainable, but not trustworthy. 

\item \textbf{Trust without explanation \& understanding}: Years before reliable explanation methods were widely acknowledged and available, black-box models, such as autoencoder and MLP neural networks, were used for fraud detection in the financial services industry \cite{gopinathan1998fraud}. When these models performed well, they were trusted.\footnote{See: \url{https://www.sas.com/en_ph/customers/hsbc.html}, \url{https://www.kdnuggets.com/2011/03/sas-patent-fraud-detection.html}.} However, they were not explainable or well-understood by contemporary standards.  

\end{itemize}

If trust in models is your goal, then explanations alone are not sufficient. However, as discussed in Sections \ref{sec:white_box} and \ref{sec:conclusion} and illustrated in Figure \ref{fig:hc_ml}, in an ideal scenario, explanation techniques would be used with a wide variety of other methods to increase accuracy, fairness, interpretability, privacy, security, and trust in ML models. 

\subsection{Guideline: Learn How Explainable ML is Used for Nefarious Purposes.} \label{sec:nefarious}

When used disingenuously, explainable ML methods can provide cover for misused or intentionally abusive black-boxes \cite{fair_washing}, \cite{please_stop}. Explainable ML methods can also enable hacking or stealing of data and models through public prediction APIs or other endpoints \cite{membership_inference}, \cite{model_stealing}. Moreover, explainable ML methods are likely to be used for other nefarious purposes in the future and may be used for unknown destructive purposes now. 

\subsubsection{Corollary: Explainable ML Can be Used to Crack Nefarious Black-boxes.} Used as white-hat hacking tools, explainable ML methods can draw attention to potential fairness or accuracy problems in proprietary black-boxes. See \citet{angwin16} for evidence that cracking of commercial black-box models for oversight purposes is possible.\footnote{This text makes no claim on the quality of the analysis in Angwin et al. (2016), which has been criticized \cite{flores2016false}. This now infamous analysis is presented only as evidence that motivated activists can crack commercial black-boxes using surrogate models and other explanatory techniques.} Such investigations would likely only be improved by advances in explanatory and fairness tools.

\subsubsection{Corollary: Explainable ML is a Privacy Vulnerability.} Recent research has shown that providing explanations along with model predictions eases attacks that can compromise sensitive training data \cite{shokri2019privacy}. When issuing predictions with explanations, reveal only as much information as needed to appeal erroneous model decisions and protect prediction APIs and other endpoints with access control and security measures. 

\subsection{Augment Surrogate Models with Direct Explanations.} \label{sec:surrogate}

Models of models, or surrogate models, can be helpful explanatory tools, but they are usually approximate, low-fidelity explainers. Aside from 1.) a global or local summary of a complex model provided by a surrogate model can be helpful sometimes and 2.) much work in explainable ML has been directed toward improving the fidelity and usefulness of surrogate models \cite{dt_surrogate2}, \cite{viper}, \cite{dt_surrogate1}, \cite{lime-sup}, \cite{wf_xnn}, \textit{many explainable ML techniques have nothing to do with surrogate models!} One of the most exciting breakthroughs for supervised learning problems in explainable ML is the application of a coalitional game theory concept, Shapley values, to compute feature attributions which are consistent globally and accurate locally using the trained model itself \cite{shapley}, \cite{kononenko2010efficient}. An extension of this idea, called tree SHAP, has already been implemented for popular tree ensemble methods \cite{tree_shap}. 

\begin{figure*}[htb!]
	\begin{subfigure}{.55\textwidth}	\centering
		\includegraphics[height=0.5\linewidth, width=0.8\linewidth]{img/dt_surrogate.png}
  		\caption{Na\"ive $h_{\text{tree}}$, \textit{a surrogate model}, forms an approximate overall flowchart\\ for the explained model, $g_{\text{GBM}}$.}
  		\label{fig:dt_surrogate}
	\end{subfigure}\hfill
	\begin{subfigure}{.45\textwidth}	\centering
  		\includegraphics[height=0.4\linewidth, width=0.75\linewidth]{img/pdp_ice.png}
  		\caption{Partial dependence and ICE curves generated \textit{directly from the explained model}, $g_{\text{GBM}}$.}
  		\label{fig:pdp_ice}
	\end{subfigure} \vspace{-5pt}
	\caption{$h_{\text{tree}}$ displays known interactions in $f = X_{\text{num}1} * X_{\text{num}4} + |X_{\text{num}8}| * X_{\text{num}9}^2$ for $\sim -0.923 < X_{\text{num9}} <  \sim 1.04$. Modeling of the known interaction between $X_{\text{num9}}$ and $X_{\text{num8}}$ in $f$ by $g_{\text{GBM}}$ is confirmed by the divergence of partial dependence and ICE curves for $\sim -1 < X_{\text{num9}} <  \sim 1$. Explanations from a surrogate model have augmented and confirmed findings from a direct model visualization technique. Code to replicate Figure \ref{fig:pdp_ice_dt_surrogate} is available here: \url{https://github.com/h2oai/xai_guidelines}.}
	\label{fig:pdp_ice_dt_surrogate}
\end{figure*}

Many other explainable ML methods operate on trained models directly such as partial dependence, ALE, and ICE plots \cite{ale_plot}, \cite{esl}, \cite{ice_plots}. Surrogate models and explanatory techniques that operate directly on trained models can also be combined, for instance by using partial dependence, ICE, and surrogate decision trees to investigate and confirm modeled interactions \cite{art_and_sci}. In Figure \ref{fig:pdp_ice_dt_surrogate}, an unconstrained $g_{\text{GBM}}$ models a known signal generating function $f$:

\begin{equation}
\label{eq:f}
f(\mathbf{X}) = \twopartdef {1} {X_{\text{num}1} * X_{\text{num}4} + |X_{\text{num}8}| * X_{\text{num}9}^2 + e \geq 0.42} {0} {X_{\text{num}1} * X_{\text{num}4} + |X_{\text{num}8}| * X_{\text{num}9}^2 + e < 0.42}
\end{equation}

\noindent where $e$ signifies random noise in the form of label switching for roughly 15\% of the training and validation observations. $g_{\text{GBM}}$ is then trained such that $g_{\text{GBM}}(\mathbf{X}) \approx f(\mathbf{X})$ in training and validation data. $h_{\text{tree}}$, displayed in Figure \ref{fig:dt_surrogate}, is extracted such that $h_{\text{tree}}(\mathbf{X}) \approx g_{\text{GBM}}(\mathbf{X}) \approx f(\mathbf{X})$ in validation data. Partial dependence and ICE plots are generated directly for $g_{\text{GBM}}$ in the same validation data and overlaid in Figure \ref{fig:pdp_ice}. The parent-child node relationship displayed between $X_{\text{num9}}$ and $X_{\text{num8}}$ for $\sim -0.923 < X_{\text{num9}} <  \sim 1.04$ in \ref{fig:dt_surrogate} and the divergence of ICE and partial dependence curves in \ref{fig:pdp_ice} for $\sim -1 < X_{\text{num9}} <  \sim 1$ help confirm and understand how $g_{\text{GBM}}$ learned the interaction between $X_{\text{num}8}$ and $X_{\text{num}9}$ in $f$. As in Figure \ref{fig:global_shap_resid}, combining different approaches provided additional, beneficial information about a complex ML model.

\subsubsection{Corollary: Augment LIME with Direct Explanations.} LIME is important, imperfect (like every other ML technique), and one of many explainable ML tools. LIME, in it's most popular implementation, uses local linear surrogate models to explain regions of complex, machine-learned response functions \cite{lime}. Like other surrogate models, LIME can be combined with model-specific methods to yield deeper insights. Consider that tree SHAP can provide locally accurate and consistent point estimates for local feature importance as in \ref{fig:shap}. LIME can then provide approximate information about modeled local linear trends around the same point. Table \ref{tab:lime} contains LIME $h_{\text{GLM}}$ coefficients for a local region of a validation set sampled from the UCI credit card data defined by \texttt{PAY\_0 > 1}, or customers with a fairly high risk of default due to late most recent payments.  $h_{\text{GLM}}$ models the predictions of a simple interpretable decision tree model, $g_{\text{tree}}$, displayed in \ref{fig:dt}. $h_{\text{GLM}}$ coefficients show linear trends between features in the sampled set $\mathbf{X}_{\text{PAY\_0} > 1}$ and $g_{\text{tree}}(\mathbf{X}_{\text{PAY\_0}> 1})$. Because $h_{GLM}$ is relatively well-fit (0.73 $R^2$) and has a logical intercept (0.77), it can be used along with Shapley values to reason about the modeled average behavior for risky customers and to differentiate the behavior of any one specific risky customer from their peers under the model. This additional information is often useful for debugging and compliance purposes.

\begin{table}[htb!]
	\caption{Coefficients for a local linear interpretable model, $h_{\text{GLM}}$, with an intercept of 0.77 and an $R^2$ of 0.73. $h_{\text{GLM}}$ is trained on a segment of the UCI credit card dataset containing higher-risk customers with late most recent repayment statuses, $\mathbf{X}_{PAY \_ 0 > 1}$, and the predictions of a simple decision tree, $g_{\text{tree}}(\mathbf{X}_{\text{PAY\_0} > 1})$. Code to replicate Table \ref{tab:lime} is available here: \url{https://github.com/h2oai/xai_guidelines}.} 
		\centering
			\footnotesize
				\begin{tabular}{ | p{2cm} | p{1.7cm} | }
				\hline
				$h_{\text{GLM}}$\newline Feature & $h_{\text{GLM}}$\newline Coefficient \\ 
				\hline
				\texttt{PAY\_0 == 4} & $0.0009$ \\
				\hline
				\texttt{PAY\_2 == 3} & $0.0065$ \\
				\hline
				\texttt{PAY\_5 == 2} & $-0.0006$ \\
				\hline
				\texttt{PAY\_6 == 2} & $0.0036$ \\
				\hline				
				\texttt{BILL\_AMT1} & $3.4339\mathrm{e}{-08}$ \\
				\hline
				\texttt{PAY\_AMT1} & $4.8062\mathrm{e}{-07}$ \\
				\hline	
				\texttt{PAY\_AMT3} & $-5.867\mathrm{e}{-07}$ \\	
				\hline	
			\end{tabular}	
  		\label{tab:lime}
\end{table}	

\subsection{Use Fully Transparent ML Mechanisms for High-Stakes Applications.} \label{sec:white_box}

Explanation, along with white-box models, model debugging, disparate impact analysis, and the documentation they enable, are often required under numerous regulatory statutes in the U.S. and E.U., and explainable ML tools are already used to document, understand, and validate different types of models in the financial services industry \cite{lime-sup}, \cite{wf_xnn}.\textsuperscript{\ref{fn:regs}, \ref{fn:Chen}} Moreover, adverse action notices are mandated under ECOA and FCRA for many credit lending, employment, and insurance decisions in the United States.\footnote{\scriptsize{See: \url{https://consumercomplianceoutlook.org/2013/second-quarter/adverse-action-notice-requirements-under-ecoa-fcra/}.}} When ML is used for such decisions today it must be explained in terms of adverse action notices.\footnote{\scriptsize{This is \textit{already} happening: \url{https://www.prnewswire.com/news-releases/new-patent-pending-technology-from-equifax-enables-configurable-ai-models-300701153.html}.}} Shapley values, and other local feature importance approaches, provide a convenient methodology to rank the contribution of input features to model decisions and potentially generate customer-specific adverse action notices. 

Aside from regulatory mandates, explanation enables logical appeal processes for automated decisions made by ML models. Consider being negatively impacted by an erroneous black-box model decision, say for instance being mistakenly denied a loan or parole. How would you argue your case for appeal without knowing how model decisions were made? According to the New York Times, a man named Glenn Rodr\'iguez found himself in this unfortunate position in a penitentiary in Upstate New York in 2016.\footnote{This too is happening \textit{today}: \url{https://www.nytimes.com/2017/06/13/opinion/how-computers-are-harming-criminal-justice.html}.}

Some may argue that, outside of regulated dealings, for a model with little or no impact on humans and that has been thoroughly and responsibly tested by knowledgeable practitioners, that explanation is \textit{really} unnecessary. While that statement appears technically true, a counter argument centers on human learning from ML models. Explanatory techniques allow users to gain insights from complex models about nonlinear or faint phenomena and complex interactions -- information that may sometimes be unlearnable by linear models. Why go through weeks, months, or years of training and deploying a production ML system, and not take a small amount of that time to learn about the model's findings?

\begin{figure*}[htb!]
	\begin{subfigure}{.55\textwidth}
		\includegraphics[height=.45\linewidth, width=1.15\linewidth]{img/dt.png}
  		\caption{Simple decision tree, $g_{\text{tree}}$, trained on the UCI credit card data to predict default with validation AUC of 0.74. The decision policy for a high-risk individual is highlighted in red.}
  		\label{fig:dt}
	\end{subfigure}\hspace*{40pt}
	\vspace{25pt}\begin{subfigure}{.45\textwidth}
  		\includegraphics[height=.5\linewidth, width=.8\linewidth]{img/shap.png}
		\caption{Locally-accurate Shapley contributions for the\\ highlighted individual's probability of default.}
  		\label{fig:shap}
	\end{subfigure}\vspace{-30pt}
	\caption{A simple decision tree, $g_{\text{tree}}$, is trained on the UCI credit card dataset to predict probability of default. $g_{\text{tree}}$ has a validation AUC of 0.74. The decision-policy for a high-risk customer is highlighted in \ref{fig:dt} and the locally-accurate Shapley contributions for this same individual's predicted probability are displayed in \ref{fig:shap}. The Shapley values are helpful because they highlight the local importance of features not on the decision path in this particular encoding of the unknown signal-generating function, i.e. $g_{\text{tree}}$, which could be underestimated by examining the decision policy alone. Code to replicate Figure \ref{fig:dt_shap} is available here: \url{https://github.com/h2oai/xai_guidelines}.} 
	\label{fig:dt_shap}
\end{figure*}

\subsubsection{Corollary: Use Interpretable Models Along with Explanation Techniques.} A few well-known publications have focused either on white-box modeling techniques (e.g. \citet{slim}, \citet{sbrl}) or on post-hoc explanations (e.g. \citet{shapley}, \citet{lime}), but the two can be used together in the context of a holistic ML workflow as illustrated in Figure \ref{fig:hc_ml}. Consider the seemingly useful example case of augmenting globally interpretable models with local post-hoc explanations. A practitioner could train a single decision tree, a globally interpretable model, then apply local explanations in the form of Shapley feature importance as illustrated in Figure \ref{fig:dt_shap}. This enables the practitioner to see accurate numeric feature contributions for each model prediction in addition to the entire directed graph of the tree. Even for interpretable models, such as linear models and decision trees, it has been shown that Shapley values present accuracy and consistency advantages over standard feature attribution methods \cite{lipovetsky2001analysis}, \cite{tree_shap}, \cite{shapley}. 

\subsubsection{Corollary: Use Explanations Along with Bias Testing and Remediation.} Like white-box models, fairness methods (e.g. \citet{feldman2015certifying}, \citet{hardt2016equality}) are often presented in different articles than post-hoc explanatory methods. However, in banks, using post-hoc explanatory tools along with disparate impact analysis is necessary to comply with model documentation guidance and with fair lending regulations.\textsuperscript{\ref{fn:Chen},}\footnote{\scriptsize{See: \url{https://www.aba.com/Compliance/Documents/FairLendingWhitePaper2017Apr.pdf}.}}\textsuperscript{,}\footnote{\scriptsize{See: \url{https://www.govinfo.gov/content/pkg/FR-1994-04-15/html/94-9214.htm}.}} To clarify, explanatory techniques should \textit{not} replace fairness methods, but in general, explanations increase transparency and understanding of model mechanisms and predictions while bias auditing and remediation increases trust that model predictions are as fair as possible. As in previous sections, trust and understanding are different but complimentary goals achieved by combining multiple approaches.

\begin{table}[htb!]
	\centering
	\caption{Basic group disparity metrics across marital statuses for monotonically constrained GBM model, $g_{\text{mono}}$, trained on the UCI credit card dataset. Code to replicate Table \ref{tab:dia} is available here: \url{https://github.com/h2oai/xai_guidelines}.} 
	\footnotesize
	\begin{tabular}{ | p{1.1cm} | p{1.1cm} | p{1.3cm} | p{1.2cm}| p{1.2cm} | p{1.2cm} | p{1.2cm} | p{1.2cm} | }
	\hline
	& Adverse\newline Impact\newline Disparity & TPR\newline Disparity & FPR\newline Disparity & FNR\newline Disparity \\ 
	\hline
	\texttt{married} & 1.00 & 1.00 & 1.00 & 1.00 \\
	\hline	
	\texttt{single} & 0.89 & 0.99 & 0.85 & 1.01 \\
	\hline	
	\texttt{divorced} & 1.01 & 0.81 & \textcolor{red}{1.25} & 1.22 \\
	\hline
	\texttt{other} & \textcolor{red}{0.26} & \textcolor{red}{0.62} & \textcolor{red}{0} & \textcolor{red}{1.44} \\
	\hline	
	\end{tabular}
	\label{tab:dia}
\end{table}

Table \ref{tab:dia} displays basic group disparity metrics for a monotonically constrained GBM model, $g_{\text{mono}}$, trained on the UCI credit card data. In this example scenario, $g_{\text{mono}}$ displays group parity according to the four-fifths rule with \texttt{married} as the reference level for single customers, but exposes potential disparate impact for \texttt{divorced} customers and customers with martial status of \texttt{other} (for which there is very little training data). Alternative models with less disparate impact or other remediation processes should be considered in such cases to increase trust ML systems. 

\subsubsection{Corollary: Explanation is Not a Frontline Fairness Tool.} In many high-stakes and commercially viable applications of explainable ML in credit lending, insurance, and employment in the U.S. that fall under FCRA, ECOA, or other applicable regulations, demographic attributes cannot be used in predictive models and thus their contribution to model predictions cannot be explained using common explainable ML techniques. Even when demographic attributes can be used in predictive models, it has been shown that explanations may not detect unwanted social bias \cite{fair_washing}. Given these known drawbacks, it is recommended that practitioners use fairness techniques to test for and remediate unwanted sociological bias, and use explanations to augment and understand bias when appropriate.  

\subsubsection{Corollary: Use Bias Testing  Along with Constrained Models.} Because unconstrained ML models have the ability to treat similar individuals differently based on small differences in their input data values, unconstrained models can cause local bias that is not detectable with standard bias testing methods that analyze group fairness \cite{dwork2012fairness}. To minimize local unwanted sociological bias when using machine learning, and to ensure standard bias testing methods are most effective, pair bias testing techniques with constrained models.

\section{Conclusion} \label{sec:conclusion}

ML systems are used today to make life-altering decisions about employment, bail, parole, and lending.\footnote{ICLR 2019 model debugging workshop CFP: \url{https://debug-ml-iclr2019.github.io/}.} The scope of decisions delegated to ML systems seems likely only to expand in the future. Many researchers and practitioners are tackling fairness, inaccuracy, privacy violations, and security vulnerabilities with a number of brilliant, but sometimes siloed, approaches. By presenting some explainable ML guidelines, this tutorial also gives examples of combining innovations from several sub-disciplines of ML research to train explainable, fair, and trustable predictive modeling systems. As proposed in Figure \ref{fig:hc_ml}, using these techniques together can create a new and more holistic approach to ML potentially better-suited for use in business- and life-critical decision support than conventional methods.

\begin{figure}[htb!]
	\begin{center}
		\includegraphics[scale=0.1]{img/hc_ml.png}
		\caption{A diagram of a proposed human-centered ML workflow in which explanations (highlighted in green) are used along with interpretable or white-box models, disparate impact analysis and remediation techniques, and other review and appeal mechanisms to create a fair, accountable, and transparent ML system.}
		\label{fig:hc_ml}
	\end{center}
\end{figure}

\section*{Software Resources}

This tutorial uses Jupyter notebooks and Python code stored in a public GitHub repository with an Apache 2.0 license \url{https://github.com/h2oai/xai_guidelines}. Notebooks will be deployed in H2O.ai's free educational cloud environment, Aquarium: \url{http://aquarium.h2o.ai}. Attendees will only need an email address (to receive a password after Aquarium registration) and to bring their laptop to access and execute the materials. 

\section*{Acknowledgements}

The author thanks Przemyslaw Biecek, Pramit Choudhary, Christoph Molnar, and numerous members of the online data science community for their helpful input and insights. 

\small
% The next two lines define the bibliography style to be used, and the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{responsible_xai}

\end{document}
